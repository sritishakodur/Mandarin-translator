This project aims to translate Chinese text to English using a Transformer model.

Dataset:
The project utilizes the WMT18 Chinese-English Translation Dataset KAGGLE.COM
. This dataset is part of the Third Conference on Machine Translation (WMT18) and contains parallel sentences in Chinese and English, making it suitable for training machine translation models.

Project Overview:
The project involves building a Transformer-based neural machine translation model to convert Chinese text into English. The Transformer architecture, introduced by Vaswani et al., has become the foundation for state-of-the-art translation systems due to its ability to handle long-range dependencies and parallelize training.

Features:
Data Preprocessing: Tokenization and preparation of Chinese-English sentence pairs.
Model Architecture: Implementation of the Transformer model for sequence-to-sequence translation tasks.
Training: Model training on the WMT18 dataset with appropriate hyperparameter tuning.
Evaluation: Assessment of the model's performance using BLEU scores and other relevant metrics.

Technologies Used:
Programming Languages: Python
Libraries: TensorFlow, NumPy, pandas
Clone the Repository: git clone https://github.com/sritishakodur/Mandarin-translator.git
