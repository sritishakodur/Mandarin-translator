# -*- coding: utf-8 -*-
"""Copy of mandarintranslator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G3Ip9woKlPi1w4FxTZboWda158Cj8Jwm
"""

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'simhei:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F690442%2F1209810%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240611%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240611T043102Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8540055a68ca0dab895d0548bcca8bbcb331986897f047304d30a9db3fb3e8cb9205edf17515e9d807176a0f9102e5e10c34f4c3c66d5972ba6adca640f79da7f3294b2962f7647334e4c94f342fc597f5933fe618897b268429de0d69763278a2df571fe8e9e4edec032d5fd7a20c6e50b32e38b38611746af3868debba9ed1fe00c76343c7c83a98455b3c2099dea2899181b0ad3e31cc5fae514b94e8731092bebde818846f3420a01639b8a5e53189f59dc77c16086cde152149c8e783fecef3d76a7532473083da357e1604a57b5a3ad15d786550808e94428df6cd652125da504cc8f3aa9c8f5250a8e56235d66ae11b181dc8358b829fefbdd5328265,translation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3022708%2F5198173%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240611%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240611T043102Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D28b665dbaefe6df1745eeef719a0596d8f4e5317bbaf69200ae6f97c64ee6262e41df86bc3199f2c9fed7d9082af8b93f176c8b2946e000575956e8f7eed6b797e49927647df4d72d12d72e4be758913e50f6be3044d678be333d9191dc74a06f0ef641c50dfc6ef1b8b267fa0dc46ef0d4428279c648a8c9507a8ab23739385a920b539489a28b6d17cce0476f367de92e86f5674b35bae3727727fab8e8c4fab693c271f9bfc6344e9c56d8acdf4b7d0e5271e220ac3b32a6b28ad28381113ec5b3a5a0584d330c740d68e215cad53c88891e8708a62b208814a78f4da94f38dc92972c3aa87ba97a09c8e312f466ae914de935895fc2b18aef2f049ff8881'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import numpy as np
import pandas as pd
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import tensorflow as tf
import matplotlib.pyplot as plt

# Check if GPU is available
if tf.test.is_gpu_available():
    print("GPU is available")
    # Set TensorFlow to use GPU
    tf.config.set_visible_devices(tf.config.list_physical_devices('GPU'), 'GPU')
    logical_devices = tf.config.list_logical_devices('GPU')
    print(len(logical_devices), "GPU(s) are available.")
else:
    print("GPU is NOT available")
    # If GPU is not available, use CPU
    tf.config.set_visible_devices(tf.config.list_physical_devices('CPU'), 'CPU')

!pip install datasets

from datasets import load_dataset
huggingface_dataset = load_dataset("swaption2009/20k-en-zh-translation-pinyin-hsk",data_dir="")
huggingface_dataset = huggingface_dataset["train"]

num_of_rows = huggingface_dataset.num_rows//5
huggingface_raw_data = pd.DataFrame(columns=["english","chinese"])
huggingface_datalist = []
print(num_of_rows)
for i in range(num_of_rows):
    english = huggingface_dataset[(i*5)+0]["text"].strip("english: ")
    chinese = huggingface_dataset[(i*5)+2]["text"].strip("mandarin: ")
#     print(english)
#     print(chinese)
#     print("="*10)
    df_row = pd.DataFrame([[english,chinese]], columns=["english", "chinese"])
    huggingface_raw_data = pd.concat([huggingface_raw_data, df_row], ignore_index=True)
display(huggingface_raw_data)

import json

def jsontodf(json_filepath):
    json_list=[]

    #read the data line by line
    with open(json_filepath, 'r') as file:
        for idx,line in enumerate(file):
            json_list.append(line.strip())

    #convert from json list to object
    json_objects = [json.loads(json_str) for json_str in json_list]

    #json object to dataframe
    df = pd.DataFrame(json_objects)
    return df

raw_data = jsontodf("/kaggle/input/translation/translation2019zh/translation2019zh_valid.json")
display(raw_data)

import re
def contains_english_or_number(text):
    pattern = r"^(?=.*[a-zA-Z])|(?=.*\d).+$"
    return bool(re.match(pattern, text))

#create a new boolean row to indicate which datarow has english char/number
raw_data["contains_english_or_number"] = raw_data["chinese"].apply(contains_english_or_number)

#filter out rows where the condition is True
filtered_df = raw_data[~raw_data["contains_english_or_number"]]

#drop the boolean columns
filtered_df = filtered_df.drop(columns=["contains_english_or_number"])

display(filtered_df)

#create a new boolean row to indicate which datarow has english char/number
huggingface_raw_data["contains_english_or_number"] = huggingface_raw_data["chinese"].apply(contains_english_or_number)

#filter out rows where the condition is True
huggingface_filtered_df = huggingface_raw_data[~huggingface_raw_data["contains_english_or_number"]]

#drop the boolean columns
huggingface_filtered_df = huggingface_filtered_df.drop(columns=["contains_english_or_number"])

display(huggingface_filtered_df)

filtered_df = pd.concat([filtered_df,huggingface_filtered_df])
# filtered_df = huggingface_filtered_df
# filtered_df["english"] = filtered_df["english"].apply(lambda x: x.lower())
display(filtered_df)

from transformers import BertTokenizer
# tokenizer_en = BertTokenizer.from_pretrained("bert-base-uncased")
tokenizer_en = BertTokenizer.from_pretrained("bert-base-cased")
tokenizer_cn = BertTokenizer.from_pretrained("bert-base-chinese")

english_seqs = filtered_df["english"].apply(lambda x: tokenizer_en.encode(x, add_special_tokens=True, padding=False))
chinese_seqs = filtered_df["chinese"].apply(lambda x: tokenizer_cn.encode(x, add_special_tokens=True, padding=False))

import math
MAX_TOKENIZE_LENGTH = max(english_seqs.str.len().max(),chinese_seqs.str.len().max()) #longest string
MAX_TOKENIZE_LENGTH = pow(2, math.ceil(math.log(MAX_TOKENIZE_LENGTH)/math.log(2))) #closest upper to the power of 2
EMBEDDING_DEPTH = 256


print(MAX_TOKENIZE_LENGTH)
print(EMBEDDING_DEPTH)

#for english to chinese translation
cn_set_start = chinese_seqs.apply(lambda x:x[:-1]) #remove [end]
cn_set_end = chinese_seqs.apply(lambda x:x[1:]) #remove [start]

#for chinese to english translation
en_set_start = english_seqs.apply(lambda x:x[:-1]) #remove [end]
en_set_end = english_seqs.apply(lambda x:x[1:]) #remove [start]

def add_padding(token_list, max_length):
    if len(token_list) < max_length:
        padding_length = max_length - len(token_list)
        token_list = token_list + [0] * padding_length
    else:
        token_list = token_list[:max_length]  # Trim to MAX_LENGTH if longer
    return token_list

#add padding
chinese_seqs = chinese_seqs.apply(lambda x: add_padding(x,MAX_TOKENIZE_LENGTH))
english_seqs = english_seqs.apply(lambda x: add_padding(x,MAX_TOKENIZE_LENGTH))

cn_set_start = cn_set_start.apply(lambda x: add_padding(x,MAX_TOKENIZE_LENGTH-1))
cn_set_end = cn_set_end.apply(lambda x: add_padding(x,MAX_TOKENIZE_LENGTH-1))

en_set_start = en_set_start.apply(lambda x: add_padding(x,MAX_TOKENIZE_LENGTH-1))
en_set_end = en_set_end.apply(lambda x: add_padding(x,MAX_TOKENIZE_LENGTH-1))

print("=====chinese tokenized data=====")
print(chinese_seqs.iloc[0])
print(cn_set_start.iloc[0])
print(cn_set_end.iloc[0])

print("=====english tokenized data=====")
print(english_seqs.iloc[0])
print(en_set_start.iloc[0])
print(en_set_end.iloc[0])

data_size = len(filtered_df)
train_size = int(0.95*data_size)
test_size = data_size - train_size
print("train_size:",train_size)
print("test_size:",test_size)

en_training_data = []
cn_training_start_data = []
cn_training_end_data = []

cn_training_data = []
en_training_start_data = []
en_training_end_data = []

cn_to_en_training_data = []

en_testing_data = []
cn_testing_start_data = []
cn_testing_end_data = []

cn_testing_data = []
en_testing_start_data = []
en_testing_end_data = []

batch_size = 64

for i in range(data_size):
    if (i < train_size):
        en_training_data.append(tf.convert_to_tensor(english_seqs.iloc[i]))
        cn_training_start_data.append(tf.convert_to_tensor(cn_set_start.iloc[i]))
        cn_training_end_data.append(tf.convert_to_tensor(cn_set_end.iloc[i]))

        cn_training_data.append(tf.convert_to_tensor(chinese_seqs.iloc[i]))
        en_training_start_data.append(tf.convert_to_tensor(en_set_start.iloc[i]))
        en_training_end_data.append(tf.convert_to_tensor(en_set_end.iloc[i]))

    else:
        en_testing_data.append(tf.convert_to_tensor(english_seqs.iloc[i]))
        cn_testing_start_data.append(tf.convert_to_tensor(cn_set_start.iloc[i]))
        cn_testing_end_data.append(tf.convert_to_tensor(cn_set_end.iloc[i]))

        cn_testing_data.append(tf.convert_to_tensor(chinese_seqs.iloc[i]))
        en_testing_start_data.append(tf.convert_to_tensor(en_set_start.iloc[i]))
        en_testing_end_data.append(tf.convert_to_tensor(en_set_end.iloc[i]))


en_to_cn_train_set = tf.data.Dataset.from_tensor_slices(((tf.convert_to_tensor(en_training_data), tf.convert_to_tensor(cn_training_start_data)),\
                                                         tf.convert_to_tensor(cn_training_end_data)))

cn_to_en_train_set = tf.data.Dataset.from_tensor_slices(((tf.convert_to_tensor(cn_training_data), tf.convert_to_tensor(en_training_start_data)),\
                                                         tf.convert_to_tensor(en_training_end_data)))

en_to_cn_train_set = en_to_cn_train_set.batch(batch_size)
cn_to_en_train_set = cn_to_en_train_set.batch(batch_size)

en_to_cn_test_set = tf.data.Dataset.from_tensor_slices(((tf.convert_to_tensor(en_testing_data), tf.convert_to_tensor(cn_testing_start_data)),\
                                                         tf.convert_to_tensor(cn_testing_end_data)))

cn_to_en_test_set = tf.data.Dataset.from_tensor_slices(((tf.convert_to_tensor(cn_testing_data), tf.convert_to_tensor(en_testing_start_data)),\
                                                         tf.convert_to_tensor(en_testing_end_data)))

en_to_cn_test_set = en_to_cn_test_set.shuffle(train_size,reshuffle_each_iteration=True)
en_to_cn_test_set = en_to_cn_test_set.batch(batch_size)
cn_to_en_test_set = cn_to_en_test_set.batch(batch_size)

print("EN to CN train set")
for (en,cn),cn_label in en_to_cn_train_set.take(1):
    print(en.shape)
    print(cn.shape)
    print(cn_label.shape)

print("CN to EN train set")
for (cn,en),en_label in cn_to_en_train_set.take(1):
    print(cn.shape)
    print(en.shape)
    print(en_label.shape)

def positional_encoding(length, depth):
    depth = depth/2
    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)
    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)

    angle_rates = 1 / (10000**depths)         # (1, depth)
    angle_rads = positions * angle_rates      # (pos, depth)

    pos_encoding = np.concatenate(
        [np.sin(angle_rads), np.cos(angle_rads)],
        axis=-1)
    return tf.cast(pos_encoding, dtype=tf.float32)

pos_encoding = positional_encoding(length=MAX_TOKENIZE_LENGTH, depth=EMBEDDING_DEPTH)

# Check the shape.
print(pos_encoding.shape)

# Plot the dimensions.
plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')
plt.ylabel('Depth')
plt.xlabel('Position')
plt.colorbar()
plt.show()

class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.d_model = d_model
        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model, mask_zero=True)
        self.pos_encoding = positional_encoding(length=MAX_TOKENIZE_LENGTH, depth=d_model)

    def compute_mask(self, *args, **kwargs):
        return self.embedding.compute_mask(*args, **kwargs)

    def call(self, x):
        length = tf.shape(x)[1]
        x = self.embedding(x)
        # This factor sets the relative scale of the embedding and positonal_encoding.
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x = x + self.pos_encoding[tf.newaxis, :length, :]
        return x

embed_cn = PositionalEmbedding(vocab_size=tokenizer_cn.vocab_size, d_model=EMBEDDING_DEPTH)
embed_en = PositionalEmbedding(vocab_size=tokenizer_en.vocab_size, d_model=EMBEDDING_DEPTH)

cn_emb = embed_cn(cn)
en_emb = embed_en(en)

#CN to EN
print(cn_emb.shape) #embedding, positional encoding
print(en_emb.shape) #embedding, positional encoding

# # print(cn_emb._keras_mask)
# print(tf.reduce_sum(tf.cast(cn_emb._keras_mask[0], tf.int32)))
# print(tf.reduce_sum(tf.cast(cn_emb._keras_mask[1], tf.int32)))
# # print(en_emb._keras_mask)
# print(tf.reduce_sum(tf.cast(en_emb._keras_mask[0], tf.int32)))
# print(tf.reduce_sum(tf.cast(en_emb._keras_mask[1], tf.int32)))

class BaseAttention(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__()
        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
        self.layernorm = tf.keras.layers.LayerNormalization()
        self.add = tf.keras.layers.Add()

class CrossAttention(BaseAttention):
    def call(self, x, context): #x = query, content = key,value pairs
        attn_output, attn_scores = self.mha(
            query=x,
            key=context,
            value=context,
            return_attention_scores=True)

        # Cache the attention scores for plotting later.
        self.last_attn_scores = attn_scores

        x = self.add([x, attn_output])
        x = self.layernorm(x)

        return x

sample_ca = CrossAttention(num_heads=2, key_dim=EMBEDDING_DEPTH)

# CN to EN
print(cn_emb.shape) #key,pair
print(en_emb.shape) #query
print(sample_ca(en_emb, cn_emb).shape)

class GlobalSelfAttention(BaseAttention):
    def call(self, x):
        attn_output = self.mha(
            query=x,
            value=x,
            key=x)
        x = self.add([x, attn_output])
        x = self.layernorm(x)
        return x

sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=EMBEDDING_DEPTH)

# CN to EN
print(cn_emb.shape)
print(sample_gsa(cn_emb).shape)

class CausalSelfAttention(BaseAttention):
    def call(self, x):
        attn_output = self.mha(
            query=x,
            value=x,
            key=x,
            use_causal_mask = True)
        x = self.add([x, attn_output])
        x = self.layernorm(x)
        return x

sample_csa = CausalSelfAttention(num_heads=2, key_dim=EMBEDDING_DEPTH)

# CN to EN
print(en_emb.shape)
print(sample_csa(en_emb).shape)

class FeedForward(tf.keras.layers.Layer):
    def __init__(self, d_model, dff, dropout_rate=0.1):
        super().__init__()
        self.seq = tf.keras.Sequential([
          tf.keras.layers.Dense(dff, activation='relu'),
          tf.keras.layers.Dense(d_model),
          tf.keras.layers.Dropout(dropout_rate)
        ])
        self.add = tf.keras.layers.Add()
        self.layer_norm = tf.keras.layers.LayerNormalization()

    def call(self, x):
        x = self.add([x, self.seq(x)])
        x = self.layer_norm(x)
        return x

sample_ffn = FeedForward(EMBEDDING_DEPTH,MAX_TOKENIZE_LENGTH)

#CN to EN
print(en_emb.shape)
print(sample_ffn(en_emb).shape)

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):
        super().__init__()

        self.self_attention = GlobalSelfAttention(
            num_heads=num_heads,
            key_dim=d_model,
            dropout=dropout_rate)

        self.ffn = FeedForward(d_model, dff)

    def call(self, x):
        x = self.self_attention(x)
        x = self.ffn(x)
        return x

sample_encoder_layer = EncoderLayer(d_model=EMBEDDING_DEPTH, num_heads=8, dff=MAX_TOKENIZE_LENGTH)

#CN to EN
print(en_emb.shape)
print(sample_encoder_layer(en_emb).shape)

class Encoder(tf.keras.layers.Layer):
    def __init__(self, *, num_layers, d_model, num_heads,
               dff, vocab_size, dropout_rate=0.1):
        super().__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.pos_embedding = PositionalEmbedding(
            vocab_size=vocab_size, d_model=d_model)

        self.enc_layers = [
            EncoderLayer(d_model=d_model,
                         num_heads=num_heads,
                         dff=dff,
                         dropout_rate=dropout_rate)
            for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(dropout_rate)

    def call(self, x):
        # `x` is token-IDs shape: (batch, seq_len)
        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.

        # Add dropout.
        x = self.dropout(x)

        for i in range(self.num_layers):
            x = self.enc_layers[i](x)

        return x  # Shape `(batch_size, seq_len, d_model)`.

# Instantiate the encoder.
sample_encoder = Encoder(num_layers=4,
                         d_model=EMBEDDING_DEPTH,
                         num_heads=8,
                         dff=MAX_TOKENIZE_LENGTH,
                         vocab_size=tokenizer_cn.vocab_size)

sample_encoder_output = sample_encoder(cn, training=False)

# Print the shape.
#CN to EN
print(cn.shape)
print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.

class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self,
               *,
               d_model,
               num_heads,
               dff,
               dropout_rate=0.1):
        super(DecoderLayer, self).__init__()

        self.causal_self_attention = CausalSelfAttention(
            num_heads=num_heads,
            key_dim=d_model,
            dropout=dropout_rate)

        self.cross_attention = CrossAttention(
            num_heads=num_heads,
            key_dim=d_model,
            dropout=dropout_rate)

        self.ffn = FeedForward(d_model, dff)

    def call(self, x, context):
        x = self.causal_self_attention(x=x)
        x = self.cross_attention(x=x, context=context)

        # Cache the last attention scores for plotting later
        self.last_attn_scores = self.cross_attention.last_attn_scores

        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.
        return x

sample_decoder_layer = DecoderLayer(d_model=EMBEDDING_DEPTH, num_heads=8, dff=MAX_TOKENIZE_LENGTH)

sample_decoder_layer_output = sample_decoder_layer(
    x=en_emb, context=cn_emb)

#CN to EN
print(en_emb.shape)
print(cn_emb.shape)
print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`

class Decoder(tf.keras.layers.Layer):
    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,
               dropout_rate=0.1):
        super(Decoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,
                                                 d_model=d_model)
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        self.dec_layers = [
            DecoderLayer(d_model=d_model, num_heads=num_heads,
                         dff=dff, dropout_rate=dropout_rate)
            for _ in range(num_layers)]

        self.last_attn_scores = None

    def call(self, x, context):
        # `x` is token-IDs shape (batch, target_seq_len)
        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)

        x = self.dropout(x)

        for i in range(self.num_layers):
            x  = self.dec_layers[i](x, context)

        self.last_attn_scores = self.dec_layers[-1].last_attn_scores

        # The shape of x is (batch_size, target_seq_len, d_model).
        return x

# Instantiate the decoder.
sample_decoder = Decoder(num_layers=4,
                         d_model=EMBEDDING_DEPTH,
                         num_heads=8,
                         dff=MAX_TOKENIZE_LENGTH,
                         vocab_size=tokenizer_en.vocab_size)

output = sample_decoder(
    x=en,
    context=cn_emb)

# Print the shapes.
#CN to EN
print(en.shape)
print(cn_emb.shape)
print(output.shape)

sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)

#for saving custom object
tf.keras.saving.get_custom_objects().clear() #clear previous objects

@tf.keras.saving.register_keras_serializable()
class Transformer(tf.keras.Model):
    def __init__(self, *, num_layers, d_model, num_heads, dff,
               input_vocab_size, target_vocab_size, dropout_rate=0.1):
        super().__init__()
        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,
                               num_heads=num_heads, dff=dff,
                               vocab_size=input_vocab_size,
                               dropout_rate=dropout_rate)

        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,
                               num_heads=num_heads, dff=dff,
                               vocab_size=target_vocab_size,
                               dropout_rate=dropout_rate)

        self.final_layer = tf.keras.layers.Dense(target_vocab_size)

    def call(self, inputs):
        # To use a Keras model with `.fit` you must pass all your inputs in the
        # first argument.
        context, x  = inputs

        context = self.encoder(context)  # (batch_size, context_len, d_model)

        x = self.decoder(x, context)  # (batch_size, target_len, d_model)

        # Final linear layer output.
        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)

        try:
          # Drop the keras mask, so it doesn't scale the losses/metrics.
          # b/250038731
            del logits._keras_mask
        except AttributeError:
            pass

        # Return the final output and the attention weights.
        return logits

num_layers = 1
d_model = EMBEDDING_DEPTH
dff = MAX_TOKENIZE_LENGTH
num_heads = 8
dropout_rate = 0.1

cn_to_en_transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=tokenizer_cn.vocab_size,
    target_vocab_size=tokenizer_en.vocab_size,
    dropout_rate=dropout_rate)

output = cn_to_en_transformer((cn, en))

print(en.shape)
print(cn.shape)
print(output.shape)

attn_scores = cn_to_en_transformer.decoder.dec_layers[-1].last_attn_scores
print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)

cn_to_en_transformer.summary()

@tf.keras.saving.register_keras_serializable()
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super().__init__()

        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps

    def __call__(self, step):
        step = tf.cast(step, dtype=tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)

        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

    def get_config(self):
        return {
            'd_model': int(self.d_model),
            'warmup_steps': int(self.warmup_steps)
        }

learning_rate = CustomSchedule(d_model)

@tf.keras.saving.register_keras_serializable()
class CustomAdam(tf.keras.optimizers.Adam):
    def __init__(self, custom_param, **kwargs):
        super(CustomAdam, self).__init__(**kwargs)
        self.custom_param = custom_param #this is the learning rate (custom schedule)

    def get_config(self):
        config = super(CustomAdam, self).get_config()
        config.update({
            'custom_param': self.custom_param
        })
        return config

optimizer = CustomAdam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)

plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))
plt.ylabel('Learning Rate')
plt.xlabel('Train Step')

@tf.keras.saving.register_keras_serializable()
def masked_loss(label, pred):
    mask = label != 0
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')
    loss = loss_object(label, pred)

    mask = tf.cast(mask, dtype=loss.dtype)
    loss *= mask

    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)
    return loss

@tf.keras.saving.register_keras_serializable()
def masked_accuracy(label, pred):
    pred = tf.argmax(pred, axis=2)
    label = tf.cast(label, pred.dtype)
    match = label == pred

    mask = label != 0

    match = match & mask

    match = tf.cast(match, dtype=tf.float32)
    mask = tf.cast(mask, dtype=tf.float32)
    return tf.reduce_sum(match)/tf.reduce_sum(mask)

cn_to_en_transformer.compile(
    loss=masked_loss,
    optimizer=optimizer,
    metrics=[masked_accuracy])

# history = cn_to_en_transformer.fit(cn_to_en_train_set, epochs=2, validation_data=cn_to_en_test_set)
history = cn_to_en_transformer.fit(cn_to_en_train_set, epochs=25, validation_data=cn_to_en_test_set)

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
display(hist.tail())

def plot_loss(history):
    plt.plot(history.history['loss'], label='loss')
    #plt.plot(history.history['val_loss'], label='val_loss')
    #   plt.ylim([0, 10])
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
def plot_accuracy(history):
    plt.plot(history.history['masked_accuracy'], label='masked_accuracy')
    #plt.plot(history.history['val_masked_accuracy'], label='val_masked_accuracy')
    #   plt.ylim([0, 10])
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

plt.figure()
plot_loss(history)
plt.figure()
plot_accuracy(history)

def print_translation(sentence, tokens, ground_truth):
    print(f'{"Input:":15s}: {sentence}')
    print(f'{"Prediction":15s}: {tokens}')
    print(f'{"Ground truth":15s}: {ground_truth}')





class Translator(tf.Module):
    def __init__(self, input_tonkenizer,output_tokenizer, transformer):
        self.input_tonkenizer = input_tonkenizer
        self.output_tokenizer = output_tokenizer
        self.transformer = transformer

    def __call__(self, sentence, max_length=MAX_TOKENIZE_LENGTH):
        # For input sentence , hence adding the `[START]` and `[END]` tokens.
        sentence = tf.constant(self.input_tonkenizer.encode(sentence, add_special_tokens=True))[tf.newaxis]
        encoder_input = sentence

        # For output sentence, initialize the output with the `[START]` token.
        start_end = self.output_tokenizer.encode("", add_special_tokens=True)
#         print(start_end)
        start = tf.constant(start_end[0],dtype=tf.int64)[tf.newaxis]
        end = tf.constant(start_end[1],dtype=tf.int64)[tf.newaxis]
#         print(start,end)

        # `tf.TensorArray` is required here (instead of a Python list), so that the
        # dynamic-loop can be traced by `tf.function`.
        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
        output_array = output_array.write(0, start)

#         print(encoder_input.shape)
#         print(output_array.shape)


        for i in tf.range(max_length):
            output = tf.transpose(output_array.stack())
#             print(output)
            predictions = self.transformer([encoder_input, output], training=False)

            # Select the last token from the `seq_len` dimension.
            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

            predicted_id = tf.argmax(predictions, axis=-1)

            # Concatenate the `predicted_id` to the output which is given to the
            # decoder as its input.
            output_array = output_array.write(i+1, predicted_id[0])

            if predicted_id == end:
                break

        output = tf.transpose(output_array.stack())
        # The output shape is `(1, tokens)`.
#         print(output.shape)
#         print(output)
        text = self.output_tokenizer.decode(output[0], skip_special_tokens=True)  # Shape: `()`.

#         tokens = self.output_tokenizer.convert_ids_to_tokens(output)[0]

        # `tf.function` prevents us from using the attention_weights that were
        # calculated on the last iteration of the loop.
        # So, recalculate them outside the loop.
        self.transformer([encoder_input, output[:,:-1]], training=False)
        attention_weights = self.transformer.decoder.last_attn_scores

        return text, attention_weights
#         return text, tokens, attention_weights

translator = Translator(tokenizer_cn,tokenizer_en, cn_to_en_transformer)

sentence = "你好，欢迎来到中国"
ground_truth = 'Hello, Welcome to China'
translated_text, attention_weights = translator(sentence)
print_translation(sentence, translated_text, ground_truth)

sentence = "早上好，很高心见到你"
ground_truth = 'Good Morning, nice to meet you'

translated_text, attention_weights = translator(sentence)
print_translation(sentence, translated_text, ground_truth)

sentence = "祝您有个美好的一天"
ground_truth = 'Have a nice day'

translated_text, attention_weights = translator(sentence)
print_translation(sentence, translated_text, ground_truth)

"""# Attention plot"""

def plot_attention_head(in_tokens, in_tokenizer, in_font, translated_tokens, translated_tokenizer, attention):
    # The model didn't generate `<START>` in the output. Skip it.
#     translated_tokens = translated_tokens[1:]
    in_tokens = in_tokens[0]
    translated_tokens = translated_tokens[0]

    in_tokens = in_tokenizer.decode(in_tokens).split(" ")
    #print(in_tokens)

    translated_tokens = translated_tokenizer.decode(translated_tokens,skip_special_tokens=True).split(" ")
    #print(translated_tokens)

    ax = plt.gca()
    ax.matshow(attention)
    ax.set_xticks(range(len(in_tokens)))
    ax.set_yticks(range(len(translated_tokens)))

    labels = [label for label in in_tokens]
    ax.set_xticklabels(
      labels, rotation=90, fontproperties=in_font)

    labels = [label for label in translated_tokens]
    ax.set_yticklabels(labels)

head = 0
# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.
attention_heads = tf.squeeze(attention_weights, 0)
attention = attention_heads[head]
print(attention_weights.shape)
print(attention.shape)
# print(attention)

in_tokens = tf.constant(tokenizer_cn.encode(sentence, add_special_tokens=True))[tf.newaxis]
print(in_tokens.shape)

out_tokens = tf.constant(tokenizer_en.encode(translated_text, add_special_tokens=False))[tf.newaxis]
print(out_tokens.shape)

from matplotlib.font_manager import FontProperties

# Specify the path to the downloaded SimHei.ttf font file
simhei_path = '/kaggle/input/simhei/simhei.ttf'

# Create a font property with SimHei font
simhei_font = FontProperties(fname=simhei_path)

plot_size = 20
plt.figure(figsize=(plot_size,plot_size))
plot_attention_head(in_tokens, tokenizer_cn, simhei_font, out_tokens, tokenizer_en, attention)

def plot_attention_weights(in_tokens, tokenizer_cn, out_tokens, tokenizer_en, attention_heads):

    plot_size = 20
    fig = plt.figure(figsize=(plot_size,plot_size))#plt.figure(figsize=(16, 8))

    for h, head in enumerate(attention_heads):
        ax = fig.add_subplot(2, 4, h+1)

        plot_attention_head(in_tokens, tokenizer_cn, simhei_font, out_tokens, tokenizer_en, head)

        ax.set_xlabel(f'Head {h+1}')

#     plt.tight_layout()
    plt.show()

plot_attention_weights(in_tokens, tokenizer_cn, out_tokens, tokenizer_en, attention_weights[0])

#model export
cn_to_en_transformer.save("cn_to_en_transformer.keras")
cn_to_en_transformer_reload = tf.keras.models.load_model("cn_to_en_transformer.keras")
print(tf.math.reduce_all(tf.equal(cn_to_en_transformer((cn, en)),cn_to_en_transformer_reload((cn, en)))))



interface = gr.Interface(fn=translate,inputs=gr.Textbox(lines=2, placeholder='Text to translate'),
                        outputs='text')
interface.launch()

